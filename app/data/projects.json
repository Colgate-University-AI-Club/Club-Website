[
  {
    "id": "p1",
    "slug": "sentiment-classifier",
    "title": "Twitter Sentiment Analysis Classifier",
    "level": "beginner",
    "durationHours": 4,
    "summary": "Build a machine learning model to classify tweets as positive, negative, or neutral using Python and scikit-learn.",
    "repoUrl": "https://github.com/Colgate-University-Ai-Club/sentiment-classifier",
    "resources": [
      {
        "label": "Google Colab",
        "url": "https://colab.research.google.com"
      },
      {
        "label": "scikit-learn Documentation",
        "url": "https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html"
      },
      {
        "label": "Twitter Dataset",
        "url": "https://www.kaggle.com/datasets/kazanova/sentiment140"
      },
      {
        "label": "Natural Language Processing Basics",
        "url": "https://realpython.com/nltk-nlp-python/"
      }
    ],
    "body": "## Project Overview\n\nSentiment analysis is one of the most popular applications of Natural Language Processing (NLP). In this beginner-friendly project, you'll build a machine learning classifier that can automatically determine whether a tweet expresses positive, negative, or neutral sentiment.\n\n## Learning Objectives\n\nBy completing this project, you will:\n- Understand the fundamentals of text classification\n- Learn how to preprocess text data for machine learning\n- Implement feature extraction using TF-IDF vectorization\n- Train and evaluate a Naive Bayes classifier\n- Visualize model performance with confusion matrices\n\n## Prerequisites\n\n- Basic Python programming knowledge\n- Familiarity with pandas and NumPy (helpful but not required)\n- Understanding of basic ML concepts (training/testing split, accuracy)\n- Google account for Colab access\n\n## Step-by-Step Guide\n\n### 1. Environment Setup\n\nOpen Google Colab and create a new notebook. Install required packages:\n\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score, classification_report\n```\n\n### 2. Load and Explore the Dataset\n\nDownload the Sentiment140 dataset from Kaggle and load it into your notebook:\n\n```python\n# Load the dataset\ndf = pd.read_csv('training.csv', encoding='latin-1', header=None)\ndf.columns = ['target', 'id', 'date', 'flag', 'user', 'text']\n\n# Explore the data\nprint(df.head())\nprint(f\"Dataset shape: {df.shape}\")\nprint(f\"Sentiment distribution:\\n{df['target'].value_counts()}\")\n```\n\n### 3. Data Preprocessing\n\nClean and prepare the text data:\n\n```python\nimport re\nimport string\n\ndef clean_text(text):\n    # Remove URLs\n    text = re.sub(r'http\\S+', '', text)\n    # Remove mentions and hashtags\n    text = re.sub(r'@\\w+|#\\w+', '', text)\n    # Remove punctuation\n    text = text.translate(str.maketrans('', '', string.punctuation))\n    # Convert to lowercase\n    text = text.lower()\n    return text\n\ndf['clean_text'] = df['text'].apply(clean_text)\n```\n\n### 4. Train-Test Split\n\n```python\nX = df['clean_text']\ny = df['target']\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n```\n\n### 5. Feature Extraction with TF-IDF\n\n```python\nvectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\nX_train_tfidf = vectorizer.fit_transform(X_train)\nX_test_tfidf = vectorizer.transform(X_test)\n```\n\n### 6. Train the Classifier\n\n```python\nclassifier = MultinomialNB()\nclassifier.fit(X_train_tfidf, y_train)\n```\n\n### 7. Evaluate Performance\n\n```python\ny_pred = classifier.predict(X_test_tfidf)\n\nprint(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred))\n```\n\n### 8. Test with Custom Tweets\n\n```python\ndef predict_sentiment(text):\n    clean = clean_text(text)\n    vec = vectorizer.transform([clean])\n    prediction = classifier.predict(vec)[0]\n    return \"Positive\" if prediction == 4 else \"Negative\"\n\n# Try it out!\ntest_tweets = [\n    \"I love this new AI project!\",\n    \"This is the worst experience ever.\",\n    \"The weather is nice today.\"\n]\n\nfor tweet in test_tweets:\n    print(f\"Tweet: {tweet}\")\n    print(f\"Sentiment: {predict_sentiment(tweet)}\\n\")\n```\n\n## Expected Outcomes\n\n- A trained sentiment classifier with 75-80% accuracy\n- Understanding of text preprocessing techniques\n- Experience with scikit-learn's ML pipeline\n- A reusable prediction function for new tweets\n\n## Next Steps\n\n- Try different classifiers (Logistic Regression, SVM)\n- Experiment with word embeddings (Word2Vec, GloVe)\n- Add neutral sentiment classification\n- Deploy your model as a web API using Flask\n\n## Troubleshooting\n\n**Issue**: Low accuracy\n- Solution: Increase max_features in TfidfVectorizer or try different preprocessing\n\n**Issue**: Out of memory errors\n- Solution: Use a smaller sample of the dataset or reduce max_features\n\n**Issue**: Slow training\n- Solution: Enable GPU runtime in Colab (Runtime > Change runtime type > GPU)"
  },
  {
    "id": "p2",
    "slug": "rag-chatbot",
    "title": "RAG-Powered Document Chatbot",
    "level": "intermediate",
    "durationHours": 6,
    "summary": "Create an intelligent chatbot that answers questions about your documents using Retrieval Augmented Generation (RAG) with LangChain and OpenAI.",
    "repoUrl": "https://github.com/Colgate-University-Ai-Club/rag-chatbot",
    "resources": [
      {
        "label": "LangChain Documentation",
        "url": "https://python.langchain.com/docs/get_started/introduction"
      },
      {
        "label": "OpenAI API Guide",
        "url": "https://platform.openai.com/docs/quickstart"
      },
      {
        "label": "Vector Database Tutorial",
        "url": "https://www.pinecone.io/learn/vector-database/"
      },
      {
        "label": "RAG Explained",
        "url": "https://research.ibm.com/blog/retrieval-augmented-generation-RAG"
      },
      {
        "label": "Streamlit Documentation",
        "url": "https://docs.streamlit.io/"
      }
    ],
    "body": "## Project Overview\n\nRetrieval Augmented Generation (RAG) is a cutting-edge technique that combines the power of large language models with your own data. In this project, you'll build a chatbot that can answer questions about PDF documents by retrieving relevant context and generating informed responses.\n\n## Learning Objectives\n\nBy completing this project, you will:\n- Understand the RAG architecture and workflow\n- Learn to work with vector embeddings and similarity search\n- Integrate OpenAI's GPT models with LangChain\n- Build a conversational interface with Streamlit\n- Implement document chunking and semantic search\n\n## Prerequisites\n\n- Intermediate Python programming skills\n- Basic understanding of APIs and asynchronous programming\n- Familiarity with machine learning concepts\n- OpenAI API key (requires paid account)\n- Experience with terminal/command line\n\n## Step-by-Step Guide\n\n### 1. Environment Setup\n\nCreate a new Python environment and install dependencies:\n\n```bash\npython -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\npip install langchain openai chromadb pypdf streamlit python-dotenv\n```\n\nCreate a `.env` file:\n\n```\nOPENAI_API_KEY=your_api_key_here\n```\n\n### 2. Document Processing Pipeline\n\nCreate `document_processor.py`:\n\n```python\nfrom langchain.document_loaders import PyPDFLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.vectorstores import Chroma\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\ndef process_documents(pdf_path):\n    # Load PDF\n    loader = PyPDFLoader(pdf_path)\n    documents = loader.load()\n    \n    # Split into chunks\n    text_splitter = RecursiveCharacterTextSplitter(\n        chunk_size=1000,\n        chunk_overlap=200,\n        length_function=len\n    )\n    chunks = text_splitter.split_documents(documents)\n    \n    # Create embeddings\n    embeddings = OpenAIEmbeddings()\n    \n    # Store in vector database\n    vectorstore = Chroma.from_documents(\n        documents=chunks,\n        embedding=embeddings,\n        persist_directory=\"./chroma_db\"\n    )\n    \n    return vectorstore\n\nif __name__ == \"__main__\":\n    # Process your documents\n    vectorstore = process_documents(\"your_document.pdf\")\n    print(f\"Processed {len(vectorstore.get()['ids'])} chunks\")\n```\n\n### 3. RAG Chain Implementation\n\nCreate `rag_chain.py`:\n\n```python\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.chains import ConversationalRetrievalChain\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.vectorstores import Chroma\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nclass RAGChatbot:\n    def __init__(self, persist_directory=\"./chroma_db\"):\n        # Initialize embeddings\n        self.embeddings = OpenAIEmbeddings()\n        \n        # Load vector database\n        self.vectorstore = Chroma(\n            persist_directory=persist_directory,\n            embedding_function=self.embeddings\n        )\n        \n        # Initialize LLM\n        self.llm = ChatOpenAI(\n            model_name=\"gpt-3.5-turbo\",\n            temperature=0.7\n        )\n        \n        # Setup memory\n        self.memory = ConversationBufferMemory(\n            memory_key=\"chat_history\",\n            return_messages=True,\n            output_key=\"answer\"\n        )\n        \n        # Create retrieval chain\n        self.chain = ConversationalRetrievalChain.from_llm(\n            llm=self.llm,\n            retriever=self.vectorstore.as_retriever(\n                search_kwargs={\"k\": 3}\n            ),\n            memory=self.memory,\n            return_source_documents=True\n        )\n    \n    def ask(self, question):\n        response = self.chain({\"question\": question})\n        return {\n            \"answer\": response[\"answer\"],\n            \"sources\": response[\"source_documents\"]\n        }\n\n# Test the chatbot\nif __name__ == \"__main__\":\n    bot = RAGChatbot()\n    \n    questions = [\n        \"What is the main topic of this document?\",\n        \"Can you summarize the key findings?\",\n        \"What are the recommendations mentioned?\"\n    ]\n    \n    for q in questions:\n        print(f\"\\nQuestion: {q}\")\n        result = bot.ask(q)\n        print(f\"Answer: {result['answer']}\")\n        print(f\"Sources: {len(result['sources'])} documents\")\n```\n\n### 4. Streamlit Web Interface\n\nCreate `app.py`:\n\n```python\nimport streamlit as st\nfrom rag_chain import RAGChatbot\nimport os\n\n# Page config\nst.set_page_config(\n    page_title=\"RAG Document Chatbot\",\n    page_icon=\"🤖\",\n    layout=\"wide\"\n)\n\nst.title(\"📚 RAG-Powered Document Chatbot\")\n\n# Initialize chatbot\n@st.cache_resource\ndef load_chatbot():\n    return RAGChatbot()\n\nbot = load_chatbot()\n\n# Initialize chat history\nif \"messages\" not in st.session_state:\n    st.session_state.messages = []\n\n# Display chat history\nfor message in st.session_state.messages:\n    with st.chat_message(message[\"role\"]):\n        st.markdown(message[\"content\"])\n\n# Chat input\nif prompt := st.chat_input(\"Ask a question about your documents...\"):\n    # Add user message\n    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n    with st.chat_message(\"user\"):\n        st.markdown(prompt)\n    \n    # Get bot response\n    with st.chat_message(\"assistant\"):\n        with st.spinner(\"Thinking...\"):\n            result = bot.ask(prompt)\n            response = result[\"answer\"]\n            st.markdown(response)\n            \n            # Show sources\n            with st.expander(\"View Sources\"):\n                for i, doc in enumerate(result[\"sources\"]):\n                    st.markdown(f\"**Source {i+1}:**\")\n                    st.text(doc.page_content[:300] + \"...\")\n    \n    # Add assistant message\n    st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n\n# Sidebar\nwith st.sidebar:\n    st.header(\"About\")\n    st.markdown(\"\"\"\n    This chatbot uses RAG (Retrieval Augmented Generation) to answer \n    questions about your documents.\n    \n    **Features:**\n    - Semantic search through documents\n    - Context-aware responses\n    - Source citation\n    - Conversation memory\n    \"\"\")\n    \n    if st.button(\"Clear Chat History\"):\n        st.session_state.messages = []\n        st.rerun()\n```\n\n### 5. Running the Application\n\n```bash\n# First, process your documents\npython document_processor.py\n\n# Then start the Streamlit app\nstreamlit run app.py\n```\n\n## Expected Outcomes\n\n- A fully functional RAG chatbot accessible via web browser\n- Ability to ask questions about uploaded documents\n- Responses with source citations\n- Conversation memory for follow-up questions\n- Understanding of vector embeddings and semantic search\n\n## Advanced Extensions\n\n1. **Multiple Document Support**: Process entire directories of PDFs\n2. **Document Upload UI**: Add file upload directly in Streamlit\n3. **Custom Prompting**: Implement system prompts for specific use cases\n4. **Evaluation Metrics**: Add answer quality scoring\n5. **Different Embeddings**: Try sentence-transformers for local embeddings\n6. **Persistent Memory**: Save conversation history to database\n\n## Troubleshooting\n\n**Issue**: OpenAI API rate limits\n- Solution: Implement exponential backoff or use gpt-3.5-turbo-16k\n\n**Issue**: Poor answer quality\n- Solution: Adjust chunk_size, increase k in retriever, or tune temperature\n\n**Issue**: Slow response times\n- Solution: Use smaller documents, reduce k value, or implement caching\n\n**Issue**: High API costs\n- Solution: Use embeddings caching, implement local LLM (Llama 2), or reduce chunk overlap\n\n## Performance Tips\n\n- Cache embeddings to avoid reprocessing\n- Use smaller chunk sizes for technical documents\n- Implement hybrid search (keyword + semantic)\n- Monitor token usage and optimize prompts\n- Consider using Pinecone for production vector storage"
  },
  {
    "id": "p3",
    "slug": "object-detection",
    "title": "Real-Time Object Detection with YOLO",
    "level": "intermediate",
    "durationHours": 5,
    "summary": "Implement a computer vision system that detects and tracks objects in real-time using YOLOv8 and OpenCV.",
    "repoUrl": "https://github.com/Colgate-University-Ai-Club/object-detection",
    "resources": [
      {
        "label": "Ultralytics YOLOv8 Docs",
        "url": "https://docs.ultralytics.com/"
      },
      {
        "label": "OpenCV Python Tutorial",
        "url": "https://docs.opencv.org/4.x/d6/d00/tutorial_py_root.html"
      },
      {
        "label": "COCO Dataset",
        "url": "https://cocodataset.org/"
      },
      {
        "label": "Computer Vision Basics",
        "url": "https://www.coursera.org/learn/computer-vision-basics"
      }
    ],
    "body": "## Project Overview\n\nObject detection is a fundamental computer vision task with applications in autonomous vehicles, surveillance, robotics, and more. This project teaches you to implement YOLOv8, one of the fastest and most accurate object detection models, to detect objects in images, videos, and live webcam feeds.\n\n## Learning Objectives\n\nBy completing this project, you will:\n- Understand the YOLO (You Only Look Once) architecture\n- Learn to work with pre-trained computer vision models\n- Process video streams with OpenCV\n- Implement real-time object tracking\n- Visualize detection results with bounding boxes and confidence scores\n\n## Prerequisites\n\n- Python programming experience\n- Basic understanding of neural networks\n- Familiarity with NumPy and image processing concepts\n- Webcam access (optional, for real-time detection)\n- GPU recommended but not required\n\n## Step-by-Step Guide\n\n### 1. Environment Setup\n\nInstall required packages:\n\n```bash\npip install ultralytics opencv-python numpy matplotlib\n```\n\nFor GPU acceleration (optional):\n\n```bash\npip install torch torchvision --index-url https://download.pytorch.org/whl/cu118\n```\n\n### 2. Basic Object Detection on Images\n\nCreate `detect_image.py`:\n\n```python\nfrom ultralytics import YOLO\nimport cv2\nimport matplotlib.pyplot as plt\n\n# Load pre-trained YOLOv8 model\nmodel = YOLO('yolov8n.pt')  # nano model (fastest)\n\ndef detect_objects(image_path):\n    # Run inference\n    results = model(image_path)\n    \n    # Get the first result\n    result = results[0]\n    \n    # Print detected objects\n    print(f\"Found {len(result.boxes)} objects:\")\n    for box in result.boxes:\n        cls_id = int(box.cls[0])\n        conf = float(box.conf[0])\n        class_name = model.names[cls_id]\n        print(f\"  - {class_name}: {conf:.2f}\")\n    \n    # Visualize results\n    annotated_img = result.plot()\n    \n    # Display\n    plt.figure(figsize=(12, 8))\n    plt.imshow(cv2.cvtColor(annotated_img, cv2.COLOR_BGR2RGB))\n    plt.axis('off')\n    plt.title('Object Detection Results')\n    plt.show()\n    \n    return result\n\n# Test with sample image\nif __name__ == \"__main__\":\n    result = detect_objects(\"sample_image.jpg\")\n```\n\n### 3. Video Object Detection\n\nCreate `detect_video.py`:\n\n```python\nfrom ultralytics import YOLO\nimport cv2\n\nmodel = YOLO('yolov8n.pt')\n\ndef process_video(video_path, output_path='output.mp4'):\n    # Open video\n    cap = cv2.VideoCapture(video_path)\n    \n    # Get video properties\n    fps = int(cap.get(cv2.CAP_PROP_FPS))\n    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    \n    # Setup video writer\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n    \n    frame_count = 0\n    \n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n        \n        # Run detection\n        results = model(frame, verbose=False)\n        \n        # Draw results\n        annotated_frame = results[0].plot()\n        \n        # Write frame\n        out.write(annotated_frame)\n        \n        # Display progress\n        frame_count += 1\n        if frame_count % 30 == 0:\n            print(f\"Processed {frame_count} frames\")\n        \n        # Optional: Display live\n        cv2.imshow('Detection', annotated_frame)\n        if cv2.waitKey(1) & 0xFF == ord('q'):\n            break\n    \n    cap.release()\n    out.release()\n    cv2.destroyAllWindows()\n    \n    print(f\"Video saved to {output_path}\")\n\nif __name__ == \"__main__\":\n    process_video(\"input_video.mp4\")\n```\n\n### 4. Real-Time Webcam Detection\n\nCreate `detect_webcam.py`:\n\n```python\nfrom ultralytics import YOLO\nimport cv2\nimport time\n\nmodel = YOLO('yolov8n.pt')\n\ndef webcam_detection():\n    cap = cv2.VideoCapture(0)  # 0 for default webcam\n    \n    # FPS calculation\n    prev_time = time.time()\n    \n    print(\"Starting webcam detection. Press 'q' to quit.\")\n    \n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n        \n        # Run detection\n        results = model(frame, verbose=False)\n        \n        # Annotate frame\n        annotated_frame = results[0].plot()\n        \n        # Calculate FPS\n        curr_time = time.time()\n        fps = 1 / (curr_time - prev_time)\n        prev_time = curr_time\n        \n        # Display FPS\n        cv2.putText(\n            annotated_frame,\n            f\"FPS: {fps:.1f}\",\n            (10, 30),\n            cv2.FONT_HERSHEY_SIMPLEX,\n            1,\n            (0, 255, 0),\n            2\n        )\n        \n        # Show frame\n        cv2.imshow('Webcam Object Detection', annotated_frame)\n        \n        # Quit on 'q' press\n        if cv2.waitKey(1) & 0xFF == ord('q'):\n            break\n    \n    cap.release()\n    cv2.destroyAllWindows()\n\nif __name__ == \"__main__\":\n    webcam_detection()\n```\n\n### 5. Custom Object Detection and Filtering\n\nCreate `custom_detection.py`:\n\n```python\nfrom ultralytics import YOLO\nimport cv2\nimport numpy as np\n\nmodel = YOLO('yolov8n.pt')\n\nclass ObjectDetector:\n    def __init__(self, target_classes=None, confidence_threshold=0.5):\n        self.model = model\n        self.target_classes = target_classes\n        self.conf_threshold = confidence_threshold\n    \n    def detect(self, frame, show_labels=True):\n        # Run inference\n        results = self.model(frame, verbose=False, conf=self.conf_threshold)\n        result = results[0]\n        \n        # Filter by target classes\n        detections = []\n        for box in result.boxes:\n            cls_id = int(box.cls[0])\n            class_name = self.model.names[cls_id]\n            \n            # Filter if target classes specified\n            if self.target_classes and class_name not in self.target_classes:\n                continue\n            \n            conf = float(box.conf[0])\n            xyxy = box.xyxy[0].cpu().numpy()\n            \n            detections.append({\n                'class': class_name,\n                'confidence': conf,\n                'bbox': xyxy\n            })\n            \n            # Draw bounding box\n            x1, y1, x2, y2 = map(int, xyxy)\n            color = (0, 255, 0)\n            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n            \n            if show_labels:\n                label = f\"{class_name}: {conf:.2f}\"\n                cv2.putText(\n                    frame,\n                    label,\n                    (x1, y1 - 10),\n                    cv2.FONT_HERSHEY_SIMPLEX,\n                    0.5,\n                    color,\n                    2\n                )\n        \n        return frame, detections\n\n# Example: Detect only people and cars\nif __name__ == \"__main__\":\n    detector = ObjectDetector(\n        target_classes=['person', 'car'],\n        confidence_threshold=0.6\n    )\n    \n    cap = cv2.VideoCapture(0)\n    \n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n        \n        annotated_frame, detections = detector.detect(frame)\n        \n        # Print detection count\n        print(f\"Detected: {len(detections)} objects\")\n        \n        cv2.imshow('Custom Detection', annotated_frame)\n        \n        if cv2.waitKey(1) & 0xFF == ord('q'):\n            break\n    \n    cap.release()\n    cv2.destroyAllWindows()\n```\n\n### 6. Model Comparison and Benchmarking\n\nCreate `benchmark_models.py`:\n\n```python\nfrom ultralytics import YOLO\nimport cv2\nimport time\n\ndef benchmark_model(model_name, test_image):\n    print(f\"\\nBenchmarking {model_name}...\")\n    model = YOLO(f\"{model_name}.pt\")\n    \n    # Warmup\n    for _ in range(3):\n        model(test_image, verbose=False)\n    \n    # Benchmark\n    start = time.time()\n    iterations = 20\n    for _ in range(iterations):\n        results = model(test_image, verbose=False)\n    end = time.time()\n    \n    avg_time = (end - start) / iterations\n    fps = 1 / avg_time\n    \n    # Get detection count\n    detections = len(results[0].boxes)\n    \n    print(f\"  Avg inference time: {avg_time*1000:.1f}ms\")\n    print(f\"  FPS: {fps:.1f}\")\n    print(f\"  Detections: {detections}\")\n    \n    return avg_time, fps\n\nif __name__ == \"__main__\":\n    test_img = \"test_image.jpg\"\n    models = ['yolov8n', 'yolov8s', 'yolov8m']\n    \n    print(\"Model Benchmark Results:\")\n    print(\"=\" * 50)\n    \n    for model_name in models:\n        benchmark_model(model_name, test_img)\n```\n\n## Expected Outcomes\n\n- Working object detection on images, videos, and live webcam\n- Understanding of YOLOv8 architecture and usage\n- Ability to filter and track specific object classes\n- Real-time performance (15-30 FPS on CPU, 60+ FPS on GPU)\n- Custom visualization and detection pipeline\n\n## Advanced Extensions\n\n1. **Object Tracking**: Implement ByteTrack or DeepSORT for multi-object tracking\n2. **Custom Training**: Train YOLOv8 on custom dataset\n3. **Pose Estimation**: Use YOLOv8-pose for human pose detection\n4. **Segmentation**: Implement instance segmentation with YOLOv8-seg\n5. **Mobile Deployment**: Export to ONNX/TFLite for mobile devices\n6. **Alert System**: Trigger alerts when specific objects detected\n\n## Troubleshooting\n\n**Issue**: Slow inference speed\n- Solution: Use smaller model (yolov8n), enable GPU, reduce input resolution\n\n**Issue**: Too many false positives\n- Solution: Increase confidence_threshold (e.g., 0.6 or 0.7)\n\n**Issue**: Webcam not detected\n- Solution: Try different camera indices (0, 1, 2) or check camera permissions\n\n**Issue**: Out of memory errors\n- Solution: Use smaller model, reduce batch size, or process lower resolution\n\n## Performance Optimization\n\n- Use GPU for 5-10x speedup\n- Optimize input resolution (640x640 default, lower for speed)\n- Use TensorRT or ONNX export for production\n- Implement frame skipping for video processing\n- Use model quantization for edge devices"
  },
  {
    "id": "p4",
    "slug": "text-summarizer",
    "title": "Automated Text Summarization Tool",
    "level": "beginner",
    "durationHours": 3,
    "summary": "Build an NLP application that automatically generates concise summaries of long articles using Hugging Face transformers.",
    "repoUrl": "https://github.com/Colgate-University-Ai-Club/text-summarizer",
    "resources": [
      {
        "label": "Hugging Face Transformers",
        "url": "https://huggingface.co/docs/transformers/index"
      },
      {
        "label": "Summarization Models",
        "url": "https://huggingface.co/models?pipeline_tag=summarization"
      },
      {
        "label": "Gradio Documentation",
        "url": "https://www.gradio.app/docs"
      },
      {
        "label": "NLP Tutorial",
        "url": "https://www.freecodecamp.org/news/natural-language-processing-tutorial/"
      }
    ],
    "body": "## Project Overview\n\nText summarization is a practical NLP application that condenses long documents into concise summaries while preserving key information. This beginner project introduces you to transformer models and shows how to build a user-friendly summarization tool with minimal code.\n\n## Learning Objectives\n\nBy completing this project, you will:\n- Understand abstractive vs. extractive summarization\n- Learn to use pre-trained transformer models from Hugging Face\n- Build interactive ML applications with Gradio\n- Work with the transformers pipeline API\n- Deploy a simple web interface for your model\n\n## Prerequisites\n\n- Basic Python programming\n- Understanding of strings and text processing\n- Familiarity with pip and virtual environments\n- No deep learning experience required!\n\n## Step-by-Step Guide\n\n### 1. Environment Setup\n\nCreate a new project directory and install dependencies:\n\n```bash\nmkdir text-summarizer\ncd text-summarizer\npython -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n\npip install transformers torch gradio newspaper3k\n```\n\n### 2. Basic Summarization Script\n\nCreate `summarize.py`:\n\n```python\nfrom transformers import pipeline\n\n# Initialize summarization pipeline\nsummarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n\ndef summarize_text(text, max_length=150, min_length=50):\n    \"\"\"\n    Generate a summary of the input text.\n    \n    Args:\n        text: Input text to summarize\n        max_length: Maximum length of summary\n        min_length: Minimum length of summary\n    \n    Returns:\n        Summary string\n    \"\"\"\n    if len(text.split()) < min_length:\n        return \"Text too short to summarize. Please provide a longer text.\"\n    \n    summary = summarizer(\n        text,\n        max_length=max_length,\n        min_length=min_length,\n        do_sample=False\n    )\n    \n    return summary[0]['summary_text']\n\n# Test with sample text\nif __name__ == \"__main__\":\n    sample_text = \"\"\"\n    Artificial intelligence (AI) is intelligence demonstrated by machines, \n    as opposed to intelligence displayed by humans or animals. Leading AI \n    textbooks define the field as the study of \"intelligent agents\": any \n    system that perceives its environment and takes actions that maximize \n    its chance of achieving its goals. Some popular accounts use the term \n    \"artificial intelligence\" to describe machines that mimic \"cognitive\" \n    functions that humans associate with the human mind, such as \"learning\" \n    and \"problem solving\", however this definition is rejected by major AI \n    researchers. AI applications include advanced web search engines, \n    recommendation systems, understanding human speech, self-driving cars, \n    automated decision-making and competing at the highest level in strategic \n    game systems. As machines become increasingly capable, tasks considered \n    to require \"intelligence\" are often removed from the definition of AI, \n    a phenomenon known as the AI effect. For instance, optical character \n    recognition is frequently excluded from things considered to be AI, \n    having become a routine technology.\n    \"\"\"\n    \n    print(\"Original text length:\", len(sample_text.split()), \"words\")\n    print(\"\\nSummary:\")\n    summary = summarize_text(sample_text)\n    print(summary)\n    print(\"\\nSummary length:\", len(summary.split()), \"words\")\n```\n\n### 3. URL Summarization Feature\n\nCreate `url_summarizer.py`:\n\n```python\nfrom newspaper import Article\nfrom transformers import pipeline\nimport re\n\nsummarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n\ndef extract_article_text(url):\n    \"\"\"\n    Extract main text content from a URL.\n    \"\"\"\n    try:\n        article = Article(url)\n        article.download()\n        article.parse()\n        return article.text, article.title\n    except Exception as e:\n        return None, None\n\ndef chunk_text(text, max_chunk_length=1024):\n    \"\"\"\n    Split text into chunks for processing.\n    \"\"\"\n    sentences = re.split(r'(?<=[.!?])\\s+', text)\n    chunks = []\n    current_chunk = []\n    current_length = 0\n    \n    for sentence in sentences:\n        sentence_length = len(sentence.split())\n        if current_length + sentence_length > max_chunk_length:\n            chunks.append(' '.join(current_chunk))\n            current_chunk = [sentence]\n            current_length = sentence_length\n        else:\n            current_chunk.append(sentence)\n            current_length += sentence_length\n    \n    if current_chunk:\n        chunks.append(' '.join(current_chunk))\n    \n    return chunks\n\ndef summarize_url(url, max_summary_length=200):\n    \"\"\"\n    Summarize an article from a URL.\n    \"\"\"\n    # Extract article\n    text, title = extract_article_text(url)\n    \n    if not text:\n        return \"Failed to extract article from URL.\"\n    \n    print(f\"Article: {title}\")\n    print(f\"Original length: {len(text.split())} words\\n\")\n    \n    # Split into chunks if necessary\n    chunks = chunk_text(text, max_chunk_length=1024)\n    \n    # Summarize each chunk\n    chunk_summaries = []\n    for i, chunk in enumerate(chunks):\n        if len(chunk.split()) < 50:\n            continue\n        \n        summary = summarizer(\n            chunk,\n            max_length=150,\n            min_length=40,\n            do_sample=False\n        )\n        chunk_summaries.append(summary[0]['summary_text'])\n    \n    # Combine summaries\n    combined_summary = ' '.join(chunk_summaries)\n    \n    # Final summarization if needed\n    if len(combined_summary.split()) > max_summary_length:\n        final_summary = summarizer(\n            combined_summary,\n            max_length=max_summary_length,\n            min_length=100,\n            do_sample=False\n        )\n        return final_summary[0]['summary_text']\n    \n    return combined_summary\n\nif __name__ == \"__main__\":\n    # Test with news article\n    url = \"https://www.bbc.com/news/technology-67890123\"  # Replace with actual URL\n    summary = summarize_url(url)\n    print(\"Summary:\")\n    print(summary)\n```\n\n### 4. Interactive Web Interface with Gradio\n\nCreate `app.py`:\n\n```python\nimport gradio as gr\nfrom transformers import pipeline\nfrom newspaper import Article\nimport re\n\n# Initialize model\nprint(\"Loading model...\")\nsummarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\nprint(\"Model loaded!\")\n\ndef summarize_text(text, max_length, min_length):\n    \"\"\"Summarize plain text input.\"\"\"\n    try:\n        if not text or len(text.split()) < min_length:\n            return \"Please provide a longer text (at least 50 words).\"\n        \n        summary = summarizer(\n            text,\n            max_length=int(max_length),\n            min_length=int(min_length),\n            do_sample=False\n        )\n        return summary[0]['summary_text']\n    except Exception as e:\n        return f\"Error: {str(e)}\"\n\ndef summarize_url(url, max_length, min_length):\n    \"\"\"Summarize article from URL.\"\"\"\n    try:\n        # Extract article\n        article = Article(url)\n        article.download()\n        article.parse()\n        text = article.text\n        \n        if not text:\n            return \"Failed to extract article content.\"\n        \n        # Summarize\n        summary = summarizer(\n            text[:4096],  # Limit input length\n            max_length=int(max_length),\n            min_length=int(min_length),\n            do_sample=False\n        )\n        \n        result = f\"**Article:** {article.title}\\n\\n\"\n        result += f\"**Summary:**\\n{summary[0]['summary_text']}\\n\\n\"\n        result += f\"**Original length:** {len(text.split())} words\\n\"\n        result += f\"**Summary length:** {len(summary[0]['summary_text'].split())} words\"\n        \n        return result\n    except Exception as e:\n        return f\"Error: {str(e)}\"\n\n# Create Gradio interface\nwith gr.Blocks(title=\"Text Summarization Tool\") as demo:\n    gr.Markdown(\n        \"\"\"\n        # 📝 Text Summarization Tool\n        \n        Automatically generate concise summaries using state-of-the-art AI.\n        \n        **Features:**\n        - Summarize plain text\n        - Summarize articles from URLs\n        - Adjust summary length\n        \"\"\"\n    )\n    \n    with gr.Tabs():\n        with gr.Tab(\"Text Input\"):\n            with gr.Row():\n                with gr.Column():\n                    text_input = gr.Textbox(\n                        label=\"Enter text to summarize\",\n                        placeholder=\"Paste your text here...\",\n                        lines=10\n                    )\n                    with gr.Row():\n                        text_max_length = gr.Slider(\n                            minimum=50,\n                            maximum=300,\n                            value=150,\n                            step=10,\n                            label=\"Max Summary Length\"\n                        )\n                        text_min_length = gr.Slider(\n                            minimum=20,\n                            maximum=100,\n                            value=50,\n                            step=10,\n                            label=\"Min Summary Length\"\n                        )\n                    text_button = gr.Button(\"Summarize Text\", variant=\"primary\")\n                \n                with gr.Column():\n                    text_output = gr.Textbox(\n                        label=\"Summary\",\n                        lines=10\n                    )\n            \n            text_button.click(\n                fn=summarize_text,\n                inputs=[text_input, text_max_length, text_min_length],\n                outputs=text_output\n            )\n        \n        with gr.Tab(\"URL Input\"):\n            with gr.Row():\n                with gr.Column():\n                    url_input = gr.Textbox(\n                        label=\"Enter article URL\",\n                        placeholder=\"https://example.com/article\"\n                    )\n                    with gr.Row():\n                        url_max_length = gr.Slider(\n                            minimum=50,\n                            maximum=300,\n                            value=200,\n                            step=10,\n                            label=\"Max Summary Length\"\n                        )\n                        url_min_length = gr.Slider(\n                            minimum=20,\n                            maximum=100,\n                            value=50,\n                            step=10,\n                            label=\"Min Summary Length\"\n                        )\n                    url_button = gr.Button(\"Summarize Article\", variant=\"primary\")\n                \n                with gr.Column():\n                    url_output = gr.Markdown(label=\"Summary\")\n            \n            url_button.click(\n                fn=summarize_url,\n                inputs=[url_input, url_max_length, url_min_length],\n                outputs=url_output\n            )\n    \n    gr.Markdown(\n        \"\"\"\n        ---\n        **Model:** BART-Large-CNN by Facebook AI\n        \n        **Tip:** For best results, use texts with at least 100 words.\n        \"\"\"\n    )\n\nif __name__ == \"__main__\":\n    demo.launch(share=False, server_name=\"127.0.0.1\", server_port=7860)\n```\n\n### 5. Running the Application\n\n```bash\npython app.py\n```\n\nOpen your browser to `http://127.0.0.1:7860`\n\n## Expected Outcomes\n\n- A working web application for text summarization\n- Ability to summarize both plain text and URLs\n- Understanding of transformer-based summarization\n- Experience with Gradio for rapid prototyping\n- Adjustable summary length controls\n\n## Advanced Extensions\n\n1. **Multiple Models**: Add dropdown to choose between different summarization models (T5, Pegasus, etc.)\n2. **Batch Processing**: Upload and summarize multiple documents\n3. **Export Options**: Save summaries as PDF or DOCX\n4. **Language Support**: Add multilingual summarization\n5. **Extractive Summarization**: Implement TextRank or LSA for comparison\n6. **Summary Evaluation**: Add ROUGE scores to evaluate quality\n\n## Troubleshooting\n\n**Issue**: Model download fails\n- Solution: Check internet connection, use `cache_dir` parameter\n\n**Issue**: Out of memory\n- Solution: Use a smaller model (distilbart-cnn) or reduce max_length\n\n**Issue**: Poor summary quality\n- Solution: Try different models (sshleifer/distilbart-cnn-12-6, google/pegasus-xsum)\n\n**Issue**: URL extraction fails\n- Solution: Some sites block scrapers - try different sites or add headers\n\n## Model Alternatives\n\n```python\n# Faster but less accurate\nsummarizer = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\")\n\n# Better for news articles\nsummarizer = pipeline(\"summarization\", model=\"google/pegasus-xsum\")\n\n# Multilingual support\nsummarizer = pipeline(\"summarization\", model=\"facebook/mbart-large-50\")\n```\n\n## Deployment Options\n\n- **Hugging Face Spaces**: Free hosting with GPU support\n- **Streamlit Cloud**: Alternative to Gradio\n- **Docker**: Containerize for easy deployment\n- **AWS/GCP**: Production deployment with auto-scaling"
  },
  {
    "id": "p5",
    "slug": "image-generator",
    "title": "AI Image Generator with Stable Diffusion",
    "level": "advanced",
    "durationHours": 8,
    "summary": "Create a powerful text-to-image generation system using Stable Diffusion with advanced features like LoRA fine-tuning and controlnet.",
    "repoUrl": "https://github.com/Colgate-University-Ai-Club/image-generator",
    "resources": [
      {
        "label": "Diffusers Library Docs",
        "url": "https://huggingface.co/docs/diffusers/index"
      },
      {
        "label": "Stable Diffusion Guide",
        "url": "https://stable-diffusion-art.com/beginners-guide/"
      },
      {
        "label": "LoRA Training Tutorial",
        "url": "https://huggingface.co/blog/lora"
      },
      {
        "label": "ControlNet Documentation",
        "url": "https://github.com/lllyasviel/ControlNet"
      },
      {
        "label": "Prompt Engineering Guide",
        "url": "https://prompthero.com/stable-diffusion-prompt-guide"
      }
    ],
    "body": "## Project Overview\n\nStable Diffusion is a revolutionary text-to-image generation model that creates high-quality images from text descriptions. This advanced project explores the full capabilities of diffusion models, including fine-tuning, controlnet guidance, and production deployment.\n\n## Learning Objectives\n\nBy completing this project, you will:\n- Master diffusion model architecture and theory\n- Implement text-to-image and image-to-image generation\n- Fine-tune models with LoRA for custom styles\n- Use ControlNet for precise image control\n- Build a production-grade API with FastAPI\n- Optimize inference speed and memory usage\n\n## Prerequisites\n\n- Strong Python programming skills\n- Understanding of deep learning and neural networks\n- Familiarity with PyTorch\n- GPU with at least 8GB VRAM (16GB recommended)\n- Experience with REST APIs and async programming\n- Linux/Unix command line proficiency\n\n## Step-by-Step Guide\n\n### 1. Environment Setup\n\nThis project requires significant computational resources:\n\n```bash\n# Create environment\nconda create -n diffusion python=3.10\nconda activate diffusion\n\n# Install PyTorch with CUDA\npip install torch torchvision --index-url https://download.pytorch.org/whl/cu118\n\n# Install diffusion libraries\npip install diffusers transformers accelerate safetensors\npip install xformers  # Memory efficient attention\n\n# Install API dependencies\npip install fastapi uvicorn python-multipart pillow\n\n# Install training dependencies\npip install bitsandbytes wandb datasets\n```\n\n### 2. Basic Text-to-Image Generation\n\nCreate `generate.py`:\n\n```python\nimport torch\nfrom diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\nfrom PIL import Image\nimport time\n\nclass ImageGenerator:\n    def __init__(self, model_id=\"stabilityai/stable-diffusion-2-1\"):\n        print(f\"Loading model: {model_id}\")\n        \n        # Load pipeline\n        self.pipe = StableDiffusionPipeline.from_pretrained(\n            model_id,\n            torch_dtype=torch.float16,\n            use_safetensors=True\n        )\n        \n        # Use DPM solver for faster inference\n        self.pipe.scheduler = DPMSolverMultistepScheduler.from_config(\n            self.pipe.scheduler.config\n        )\n        \n        # Enable memory optimizations\n        self.pipe.enable_attention_slicing()\n        self.pipe.enable_vae_slicing()\n        \n        # Move to GPU\n        self.pipe = self.pipe.to(\"cuda\")\n        \n        # Enable xformers if available\n        try:\n            self.pipe.enable_xformers_memory_efficient_attention()\n            print(\"✓ xformers enabled\")\n        except:\n            print(\"⚠ xformers not available\")\n        \n        print(\"Model loaded successfully!\")\n    \n    def generate(\n        self,\n        prompt: str,\n        negative_prompt: str = \"blurry, bad quality, distorted\",\n        num_images: int = 1,\n        steps: int = 25,\n        guidance_scale: float = 7.5,\n        width: int = 512,\n        height: int = 512,\n        seed: int = None\n    ):\n        \"\"\"Generate images from text prompt.\"\"\"\n        \n        # Set seed for reproducibility\n        if seed is not None:\n            generator = torch.Generator(\"cuda\").manual_seed(seed)\n        else:\n            generator = None\n        \n        print(f\"\\nGenerating {num_images} image(s)...\")\n        print(f\"Prompt: {prompt}\")\n        print(f\"Steps: {steps}, Guidance: {guidance_scale}\")\n        \n        start_time = time.time()\n        \n        # Generate\n        with torch.autocast(\"cuda\"):\n            output = self.pipe(\n                prompt=prompt,\n                negative_prompt=negative_prompt,\n                num_images_per_prompt=num_images,\n                num_inference_steps=steps,\n                guidance_scale=guidance_scale,\n                width=width,\n                height=height,\n                generator=generator\n            )\n        \n        elapsed = time.time() - start_time\n        print(f\"Generation completed in {elapsed:.2f}s\")\n        \n        return output.images\n    \n    def save_images(self, images, prefix=\"output\"):\n        \"\"\"Save generated images.\"\"\"\n        for i, img in enumerate(images):\n            filename = f\"{prefix}_{i}_{int(time.time())}.png\"\n            img.save(filename)\n            print(f\"Saved: {filename}\")\n\n# Example usage\nif __name__ == \"__main__\":\n    generator = ImageGenerator()\n    \n    prompts = [\n        \"A serene mountain landscape at sunset, oil painting style, highly detailed\",\n        \"Futuristic cyberpunk city with neon lights, rain, cinematic lighting\",\n        \"Cute robot reading a book in a cozy library, digital art\",\n    ]\n    \n    for prompt in prompts:\n        images = generator.generate(\n            prompt=prompt,\n            num_images=2,\n            steps=30,\n            guidance_scale=8.0,\n            seed=42\n        )\n        generator.save_images(images, prefix=prompt[:20].replace(\" \", \"_\"))\n```\n\n### 3. Image-to-Image Generation\n\nCreate `img2img.py`:\n\n```python\nimport torch\nfrom diffusers import StableDiffusionImg2ImgPipeline\nfrom PIL import Image\n\nclass Img2ImgGenerator:\n    def __init__(self, model_id=\"stabilityai/stable-diffusion-2-1\"):\n        self.pipe = StableDiffusionImg2ImgPipeline.from_pretrained(\n            model_id,\n            torch_dtype=torch.float16\n        )\n        self.pipe = self.pipe.to(\"cuda\")\n        self.pipe.enable_attention_slicing()\n    \n    def transform(\n        self,\n        image_path: str,\n        prompt: str,\n        strength: float = 0.75,\n        guidance_scale: float = 7.5,\n        steps: int = 50\n    ):\n        \"\"\"\n        Transform an existing image based on prompt.\n        \n        Args:\n            image_path: Path to input image\n            prompt: Text description of desired output\n            strength: How much to transform (0.0-1.0)\n            guidance_scale: How closely to follow prompt\n            steps: Number of diffusion steps\n        \"\"\"\n        # Load and prepare image\n        init_image = Image.open(image_path).convert(\"RGB\")\n        init_image = init_image.resize((512, 512))\n        \n        print(f\"Transforming image with strength {strength}\")\n        print(f\"Prompt: {prompt}\")\n        \n        # Generate\n        with torch.autocast(\"cuda\"):\n            output = self.pipe(\n                prompt=prompt,\n                image=init_image,\n                strength=strength,\n                guidance_scale=guidance_scale,\n                num_inference_steps=steps\n            )\n        \n        return output.images[0]\n\nif __name__ == \"__main__\":\n    generator = Img2ImgGenerator()\n    \n    # Transform a photo into different styles\n    result = generator.transform(\n        image_path=\"input.jpg\",\n        prompt=\"Van Gogh style painting with swirling brushstrokes\",\n        strength=0.8,\n        steps=50\n    )\n    \n    result.save(\"styled_output.png\")\n```\n\n### 4. ControlNet Integration\n\nCreate `controlnet_generator.py`:\n\n```python\nimport torch\nimport cv2\nimport numpy as np\nfrom PIL import Image\nfrom diffusers import (\n    StableDiffusionControlNetPipeline,\n    ControlNetModel,\n    UniPCMultistepScheduler\n)\nfrom controlnet_aux import CannyDetector\n\nclass ControlNetGenerator:\n    def __init__(self):\n        print(\"Loading ControlNet...\")\n        \n        # Load ControlNet model\n        controlnet = ControlNetModel.from_pretrained(\n            \"lllyasviel/sd-controlnet-canny\",\n            torch_dtype=torch.float16\n        )\n        \n        # Load pipeline\n        self.pipe = StableDiffusionControlNetPipeline.from_pretrained(\n            \"runwayml/stable-diffusion-v1-5\",\n            controlnet=controlnet,\n            torch_dtype=torch.float16\n        )\n        \n        self.pipe.scheduler = UniPCMultistepScheduler.from_config(\n            self.pipe.scheduler.config\n        )\n        self.pipe = self.pipe.to(\"cuda\")\n        self.pipe.enable_model_cpu_offload()\n        \n        # Initialize edge detector\n        self.canny = CannyDetector()\n        \n        print(\"ControlNet loaded!\")\n    \n    def generate_with_canny(\n        self,\n        image_path: str,\n        prompt: str,\n        num_images: int = 1,\n        controlnet_conditioning_scale: float = 1.0\n    ):\n        \"\"\"\n        Generate images using Canny edge detection as control.\n        \"\"\"\n        # Load input image\n        image = Image.open(image_path)\n        image = image.resize((512, 512))\n        \n        # Extract edges\n        canny_image = self.canny(image)\n        \n        # Generate\n        print(f\"Generating with ControlNet guidance...\")\n        output = self.pipe(\n            prompt=prompt,\n            image=canny_image,\n            num_images_per_prompt=num_images,\n            controlnet_conditioning_scale=controlnet_conditioning_scale,\n            num_inference_steps=30\n        )\n        \n        return output.images, canny_image\n\nif __name__ == \"__main__\":\n    generator = ControlNetGenerator()\n    \n    images, edges = generator.generate_with_canny(\n        image_path=\"sketch.jpg\",\n        prompt=\"professional photo of a modern house, architectural photography\",\n        num_images=3,\n        controlnet_conditioning_scale=0.8\n    )\n    \n    edges.save(\"edges.png\")\n    for i, img in enumerate(images):\n        img.save(f\"controlnet_output_{i}.png\")\n```\n\n### 5. Production API with FastAPI\n\nCreate `api.py`:\n\n```python\nfrom fastapi import FastAPI, File, UploadFile, Form, HTTPException\nfrom fastapi.responses import StreamingResponse\nfrom pydantic import BaseModel\nimport torch\nfrom diffusers import StableDiffusionPipeline\nfrom PIL import Image\nimport io\nimport base64\nfrom typing import Optional\n\napp = FastAPI(title=\"Stable Diffusion API\")\n\n# Initialize model on startup\n@app.on_event(\"startup\")\nasync def load_model():\n    global pipe\n    print(\"Loading model...\")\n    pipe = StableDiffusionPipeline.from_pretrained(\n        \"stabilityai/stable-diffusion-2-1\",\n        torch_dtype=torch.float16\n    )\n    pipe = pipe.to(\"cuda\")\n    pipe.enable_attention_slicing()\n    print(\"Model ready!\")\n\nclass GenerationRequest(BaseModel):\n    prompt: str\n    negative_prompt: Optional[str] = \"\"\n    steps: int = 25\n    guidance_scale: float = 7.5\n    width: int = 512\n    height: int = 512\n    seed: Optional[int] = None\n\n@app.post(\"/generate\")\nasync def generate_image(request: GenerationRequest):\n    \"\"\"\n    Generate image from text prompt.\n    \"\"\"\n    try:\n        # Set seed\n        generator = None\n        if request.seed is not None:\n            generator = torch.Generator(\"cuda\").manual_seed(request.seed)\n        \n        # Generate\n        with torch.autocast(\"cuda\"):\n            output = pipe(\n                prompt=request.prompt,\n                negative_prompt=request.negative_prompt,\n                num_inference_steps=request.steps,\n                guidance_scale=request.guidance_scale,\n                width=request.width,\n                height=request.height,\n                generator=generator\n            )\n        \n        # Convert to bytes\n        image = output.images[0]\n        img_byte_arr = io.BytesIO()\n        image.save(img_byte_arr, format='PNG')\n        img_byte_arr.seek(0)\n        \n        return StreamingResponse(img_byte_arr, media_type=\"image/png\")\n    \n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n@app.get(\"/health\")\nasync def health_check():\n    return {\"status\": \"healthy\", \"model_loaded\": pipe is not None}\n\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n```\n\nRun the API:\n\n```bash\npython api.py\n```\n\nTest with curl:\n\n```bash\ncurl -X POST \"http://localhost:8000/generate\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"prompt\": \"A beautiful sunset over mountains\",\n    \"steps\": 30,\n    \"guidance_scale\": 8.0\n  }' \\\n  --output generated.png\n```\n\n### 6. LoRA Fine-tuning Script\n\nCreate `train_lora.py`:\n\n```python\nimport torch\nfrom diffusers import StableDiffusionPipeline\nfrom peft import LoraConfig, get_peft_model\nimport argparse\n\ndef train_lora(\n    base_model: str,\n    dataset_path: str,\n    output_dir: str,\n    rank: int = 4,\n    epochs: int = 100\n):\n    \"\"\"\n    Fine-tune Stable Diffusion with LoRA.\n    \n    This is a simplified example. For production training,\n    use the official diffusers training scripts.\n    \"\"\"\n    print(f\"Training LoRA with rank {rank}...\")\n    \n    # Load base model\n    pipe = StableDiffusionPipeline.from_pretrained(\n        base_model,\n        torch_dtype=torch.float16\n    )\n    \n    # Configure LoRA\n    lora_config = LoraConfig(\n        r=rank,\n        lora_alpha=rank,\n        target_modules=[\"to_k\", \"to_q\", \"to_v\", \"to_out.0\"],\n        lora_dropout=0.1\n    )\n    \n    # Apply LoRA to UNet\n    pipe.unet = get_peft_model(pipe.unet, lora_config)\n    \n    print(f\"Trainable parameters: {pipe.unet.print_trainable_parameters()}\")\n    \n    # Training loop would go here\n    # See: https://github.com/huggingface/diffusers/tree/main/examples/dreambooth\n    \n    # Save LoRA weights\n    pipe.unet.save_pretrained(output_dir)\n    print(f\"LoRA weights saved to {output_dir}\")\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--base_model\", default=\"stabilityai/stable-diffusion-2-1\")\n    parser.add_argument(\"--dataset\", required=True)\n    parser.add_argument(\"--output\", default=\"./lora_weights\")\n    parser.add_argument(\"--rank\", type=int, default=4)\n    args = parser.parse_args()\n    \n    train_lora(\n        base_model=args.base_model,\n        dataset_path=args.dataset,\n        output_dir=args.output,\n        rank=args.rank\n    )\n```\n\n## Expected Outcomes\n\n- Production-ready image generation API\n- Understanding of diffusion model architecture\n- Experience with multiple generation modes (text2img, img2img, controlnet)\n- Ability to fine-tune models for custom styles\n- Optimized inference pipeline with memory efficiency\n- RESTful API for integration with other applications\n\n## Advanced Extensions\n\n1. **Inpainting**: Implement selective image editing\n2. **Multi-ControlNet**: Combine multiple control inputs\n3. **SDXL**: Upgrade to Stable Diffusion XL for better quality\n4. **Video Generation**: Extend to text-to-video with AnimateDiff\n5. **Prompt Embeddings**: Implement textual inversion\n6. **Queue System**: Add Celery for async job processing\n7. **Monitoring**: Integrate Prometheus metrics\n8. **Watermarking**: Add invisible watermarks to outputs\n\n## Troubleshooting\n\n**Issue**: CUDA out of memory\n- Solution: Enable attention slicing, reduce batch size, use model offloading\n\n**Issue**: Slow generation\n- Solution: Use DPM++ solver (fewer steps), enable xformers, optimize CUDA\n\n**Issue**: Poor image quality\n- Solution: Increase steps (50-100), adjust guidance scale (7-15), better prompts\n\n**Issue**: NSFW filter blocking outputs\n- Solution: Disable safety checker (use responsibly) or adjust prompts\n\n## Performance Optimization\n\n```python\n# Memory optimizations\npipe.enable_attention_slicing(\"max\")\npipe.enable_vae_slicing()\npipe.enable_model_cpu_offload()\n\n# Speed optimizations\npipe.enable_xformers_memory_efficient_attention()\npipe.unet = torch.compile(pipe.unet, mode=\"reduce-overhead\", fullgraph=True)\n\n# Use faster scheduler\nfrom diffusers import DPMSolverMultistepScheduler\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n```\n\n## Deployment Considerations\n\n- Use model quantization (bitsandbytes) for reduced VRAM\n- Implement request queuing for multiple users\n- Add rate limiting and authentication\n- Cache frequent prompts\n- Monitor GPU utilization with nvidia-smi\n- Consider serverless GPU (Modal, Banana, Replicate)"
  }
]
